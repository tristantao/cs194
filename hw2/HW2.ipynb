{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "CS194-16 Introduction to Data Science\n",
      "\n",
      "**Name**: Yuqi Tao\n",
      "\n",
      "**Student ID**: 22122190\n",
      "\n",
      "\n",
      "Assignment 2: Introduction to Machine Learning: Clustering and Regression\n",
      "===\n",
      "\n",
      "## Overview\n",
      "\n",
      "In this assignment, we will use machine learning techniques to perform data analysis and learn models about our data. We will use a real world music dataset from [Last.fm](http://last.fm) for this assignment. There are two parts to this assignment: In the first part we will look at Unsupervised Learning with clustering and in the second part, we will study Supervised Learning. The play data (and user/artist matrix) comes from the [Last.fm 1K Users dataset](http://www.dtic.upf.edu/~ocelma/MusicRecommendationDataset/lastfm-1K.html), while the tags come from [the Last.fm Music Tags dataset](http://musicmachinery.com/2010/11/10/lastfm-artisttags2007/). You won't have to interact with these datasets directly, because we've already preprocessed them for you.\n",
      "\n",
      ">**Note**: Before you begin, you should install pydot by running `sudo apt-get install python-pydot`. Additionally, you should be running on a VM with at least 1GB of RAM allotted to it - less than that and you may run into issues with scikit-learn."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Introduction to Machine Learning\n",
      "\n",
      "Machine learning is a branch of artifical intelligence where we try to find hidden structure within data. \n",
      "For example, lets say you are hired as a data scientist at a cool new music playing startup. You are given access to \n",
      "logs from the product and are asked find out what kinds of music are played on your website and how you can promote songs that will be \n",
      "popular. In this case we wish to extract some structure from the raw data we have using machine learning.\n",
      "\n",
      "There are two main kinds of machine learning algorithms:\n",
      "\n",
      "1. Unsupervised Learning - is the branch where we don't have any ground truth (or labeled data) that can help our training process. There are many approaches to unsupervised learning which includes topics like Clustering,  Mixture Models, Hidden Markov Models etc. In this assignment we will predominantly look at clustering.\n",
      "2. Supervised Learning - we have training data which is labeled (either manually or from historical data) and we try to make predictions about those labels on new, unlabeled, data. There are similarly several approaches to supervised learning - various classification and regression techniques all the way up to Support Vector Machines and Convolutional Neural Networks. In this assignment we'll explore two regression algorithms - Least Squares Linear Regression and Regression Trees. \n",
      "\n",
      "Many of the techniques you'll be using (like testing on a validation set) are of critical importance to the modeling process, regardless of the technique you're using, so keep these in mind in your future modeling efforts."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Application\n",
      "\n",
      "Your assignment is to use machine learning algorithms for two tasks on a real world music dataset from Last.fm. The goal in the first part is to cluster artists and try to discover all artists that belong a certain genre. In the second part, we'll use the same dataset and attempt to predict how popular a song will be based on a number of features of that song. One component will involve incorporating cluster information into the models."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Files\n",
      "\n",
      "Data files for this assignment can be found at:\n",
      "\n",
      "`https://github.com/amplab/datascience-sp14/raw/master/hw2/hw2data.tar.gz`\n",
      "\n",
      "The zip file includes the following files:\n",
      "\n",
      "* **artists-tags.txt**, User-defined tags for top artists\n",
      "* **userart-mat-training.csv**, Training data containing a matrix mapping artist-id to users who have played songs by the artists\n",
      "* **userart-mat-test.csv**, Test data containing a matrix mapping artist-id to users who have played songs by the artists\n",
      "* **train_model_data.csv**, Aggregate statsitstics and features about songs we'll use to train regression models.\n",
      "* **validation_model_data.csv**, Similar statistics computed on a hold-out set of users and songs that we'll use to validate our regression models.\n",
      "\n",
      "We will explain the datasets and how they need to used in the assignment sections."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Deliverables\n",
      "\n",
      "Complete the all the exercises below and turn in a write up in the form of an IPython notebook, that is, **an .ipynb file**.\n",
      "The write up should include your code, answers to exercise questions, and plots of results.\n",
      "Complete submission instructions will be posted on Piazza.\n",
      "\n",
      "We recommend that you do your work in a **copy of this notebook**, in case there are changes that need to be made that are pushed out via github. In this notebook, we provide code templates for many of the exercises. They are intended to help with code re-use, since the exercises build on each other, and are highly recommended. Don't forget to include answers to questions that ask for natural language responses, i.e., in English, not code!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Guidelines\n",
      "\n",
      "#### Code\n",
      "\n",
      "This assignment can be done with basic python, matplotlib and scikit-learn.\n",
      "Feel free to use Pandas, too, which you may find well suited to several exercises.\n",
      "As for other libraries, please check with course staff whether they're allowed.\n",
      "\n",
      "You're not required to do your coding in IPython, so feel free to use your favorite editor or IDE.\n",
      "But when you're done, remember to put your code into a notebook for your write up.\n",
      "\n",
      "#### Collaboration\n",
      "\n",
      "This assignment is to be done individually.  Everyone should be getting a hands on experience in this course.  You are free to discuss course material with fellow students, and we encourage you to use Internet resources to aid your understanding, but the work you turn in, including all code and answers, must be your own work."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Part 0: Preliminaries\n",
      "\n",
      "### Exercise 0\n",
      "\n",
      "Download the data and unzip it. \n",
      "\n",
      "Read in the file **artists-tags.txt** and store the contents in a DataFrame. The file format for this file is `artist-id|artist-name|tag|count`. The fields mean the following:\n",
      "\n",
      "1. artist-id : a unique id for an artist (Formatted as a [MusicBrainz Identifier](https://musicbrainz.org/doc/MusicBrainz_Identifier))\n",
      "2. artist-name: name of the artist\n",
      "3. tag: user-defined tag for the artist\n",
      "4. count: number of times the tag was applied\n",
      "\n",
      "Similarly, read in the file **userart-mat-training.csv** . The file format for this file is `artist-id, user1, user2, .... user1000`. i.e. There are 846 such columns in this file and each column has a value 1 if the particular user played a song from this artist."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "from pylab import *\n",
      "%matplotlib inline\n",
      "\n",
      "DATA_PATH = \"/Users/t-rex-Box/Desktop/work/cs194/hw2\" # Make this the /path/to/the/data\n",
      "\n",
      "def parse_artists_tags(filename):\n",
      "    df = pd.read_csv(filename, sep=\"|\", names=[\"ArtistID\", \"ArtistName\", \"Tag\", \"Count\"])\n",
      "    return df\n",
      "\n",
      "def parse_user_artists_matrix(filename):\n",
      "    df = pd.read_csv(filename)\n",
      "    return df\n",
      "\n",
      "artists_tags = parse_artists_tags(DATA_PATH + \"/artists-tags.txt\")\n",
      "user_art_mat = parse_user_artists_matrix(DATA_PATH + \"/userart-mat-training.csv\")\n",
      "\n",
      "print artists_tags.head()\n",
      "print user_art_mat.head()\n",
      "print \"Number of tags %d\" % len(artists_tags) # Change this line. Should be 952803\n",
      "print \"Number of artists %d\" % len(user_art_mat) # Change this line. Should be 17119"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "                               ArtistID                    ArtistName  \\\n",
        "0  000077f7-26b1-4710-80cc-f6beddbdd157  Ryan Adams and The Cardinals   \n",
        "1  000077f7-26b1-4710-80cc-f6beddbdd157  Ryan Adams and The Cardinals   \n",
        "2  000077f7-26b1-4710-80cc-f6beddbdd157  Ryan Adams and The Cardinals   \n",
        "3  00034ede-a1f1-4219-be39-02f36853373e                       O Rappa   \n",
        "4  00034ede-a1f1-4219-be39-02f36853373e                       O Rappa   \n",
        "\n",
        "                                    Tag  Count  \n",
        "0  I love you baby can I have some more      1  \n",
        "1                           alt country      2  \n",
        "2                                  whoa      1  \n",
        "3                                Artist      1  \n",
        "4                                 Black      1  \n",
        "\n",
        "[5 rows x 4 columns]\n",
        "                               ArtistID  user_000001  user_000002  \\\n",
        "0  c489cd1c-d8d3-4dcc-b7b9-dcf7cd95d2ed            0            0   \n",
        "1  9ccbb935-33fd-4df8-ae2d-779497c5630a            0            0   \n",
        "2  c71b2cc1-2737-4f76-bbf2-9ee505b4d90d            0            0   \n",
        "3  0aa8294b-6332-4b65-b677-e3a1f8591d3b            0            0   \n",
        "4  67673557-2310-41d3-83b9-e6e0cb1d65d5            0            0   \n",
        "\n",
        "   user_000003  user_000004  user_000005  user_000006  user_000007  \\\n",
        "0            0            0            0            0            0   \n",
        "1            0            0            0            0            0   \n",
        "2            0            0            0            0            0   \n",
        "3            0            0            0            0            0   \n",
        "4            0            0            0            0            0   \n",
        "\n",
        "   user_000008  user_000009  user_000010  user_000011  user_000012  \\\n",
        "0            0            0            0            0            0   \n",
        "1            0            0            0            0            0   \n",
        "2            0            0            0            0            0   \n",
        "3            0            0            0            0            1   \n",
        "4            0            0            0            0            0   \n",
        "\n",
        "   user_000013  user_000014  user_000015  user_000016  user_000017  \\\n",
        "0            0            0            0            0            0   \n",
        "1            0            0            0            0            0   \n",
        "2            0            0            0            0            1   \n",
        "3            0            0            0            0            1   \n",
        "4            0            0            0            0            0   \n",
        "\n",
        "   user_000018  user_000019      \n",
        "0            0            0 ...  \n",
        "1            0            0 ...  \n",
        "2            0            0 ...  \n",
        "3            0            0 ...  \n",
        "4            0            0 ...  \n",
        "\n",
        "[5 rows x 847 columns]\n",
        "Number of tags 952810\n",
        "Number of artists 17119\n"
       ]
      }
     ],
     "prompt_number": 107
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Part 1: Finding genres by clustering\n",
      "\n",
      "The first task we will look at is how to discover artist genres by only looking at data from plays on Last.fm. One of the ways to do this is to use clustering. To evaluate how well our clustering algorithm performs we will use the user-generated tags and compare those to our clustering results. \n",
      "\n",
      "### 1.1 Data pre-processing\n",
      "\n",
      "Last.fm allows users to associate tags with every artist (See the [top tags](http://www.last.fm/charts/toptags) for a live example). However as there are a number of tags associated with every artists, in the first step we will pre-process the data and get the most popular tag for an artist."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Exercise 1\n",
      "\n",
      "**a**. For every artist in **artists_tags** calculate the most frequently used tag. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO Implement this. You can change the function arguments if necessary\n",
      "# Return a data structure that contains (artist id, artist name, top tag) for every artist\n",
      "def calculate_top_tag(all_tags):\n",
      "    grouped = all_tags.groupby(\"ArtistID\")\n",
      "    result = {}\n",
      "    for name, group in grouped:\n",
      "        ArtistId = name\n",
      "        ArtistName = group[\"ArtistName\"].irow(0)\n",
      "        MaxTag = group[\"Tag\"][group['Count'].argmax()]\n",
      "        result[tuple([ArtistId, ArtistName])] = MaxTag\n",
      "    return result\n",
      "    #return grouped.Tag.count().idxmax()\n",
      "\n",
      "top_tags = calculate_top_tag(artists_tags)\n",
      "\n",
      "# Print the top tag for Nirvana\n",
      "# Artist ID for Nirvana is 5b11f4ce-a62d-471e-81fc-a69a8278c7da\n",
      "# Should be 'Grunge'\n",
      "print \"Top tag for Nirvana is %s\" % top_tags[(\"5b11f4ce-a62d-471e-81fc-a69a8278c7da\",\"Nirvana\")] # Complete this line "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Top tag for Nirvana is Grunge\n"
       ]
      }
     ],
     "prompt_number": 97
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**b**. To do clustering we will be using `numpy` matrices. Create a matrix from **user_art_mat** with every row in the matrix representing a single artist. The matrix will have 846 columns, one for whether each user listened to the artist."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def create_user_matrix(input_data):\n",
      "    mat = input_data.as_matrix()\n",
      "    mat = np.delete(mat,0,1) #remove first col\n",
      "    return mat\n",
      "    #print input_data.transpose().tail()\n",
      "    #mat = np.zeros(shape=(len(input_data),846))\n",
      "    #return mat\n",
      "\n",
      "user_np_matrix = create_user_matrix(user_art_mat)\n",
      "print user_np_matrix.shape # Should be (17119, 846)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "                               ArtistID  user_000001  user_000002  \\\n",
        "0  c489cd1c-d8d3-4dcc-b7b9-dcf7cd95d2ed            0            0   \n",
        "1  9ccbb935-33fd-4df8-ae2d-779497c5630a            0            0   \n",
        "2  c71b2cc1-2737-4f76-bbf2-9ee505b4d90d            0            0   \n",
        "3  0aa8294b-6332-4b65-b677-e3a1f8591d3b            0            0   \n",
        "4  67673557-2310-41d3-83b9-e6e0cb1d65d5            0            0   \n",
        "\n",
        "   user_000003  user_000004  user_000005  user_000006  user_000007  \\\n",
        "0            0            0            0            0            0   \n",
        "1            0            0            0            0            0   \n",
        "2            0            0            0            0            0   \n",
        "3            0            0            0            0            0   \n",
        "4            0            0            0            0            0   \n",
        "\n",
        "   user_000008  user_000009  user_000010  user_000011  user_000012  \\\n",
        "0            0            0            0            0            0   \n",
        "1            0            0            0            0            0   \n",
        "2            0            0            0            0            0   \n",
        "3            0            0            0            0            1   \n",
        "4            0            0            0            0            0   \n",
        "\n",
        "   user_000013  user_000014  user_000015  user_000016  user_000017  \\\n",
        "0            0            0            0            0            0   \n",
        "1            0            0            0            0            0   \n",
        "2            0            0            0            0            1   \n",
        "3            0            0            0            0            1   \n",
        "4            0            0            0            0            0   \n",
        "\n",
        "   user_000018  user_000019      \n",
        "0            0            0 ...  \n",
        "1            0            0 ...  \n",
        "2            0            0 ...  \n",
        "3            0            0 ...  \n",
        "4            0            0 ...  \n",
        "\n",
        "[5 rows x 847 columns]\n",
        "(17119, 846)\n"
       ]
      }
     ],
     "prompt_number": 163
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 1.2 K-Means clustering\n",
      "\n",
      "Having pre-processed the data we can now perform clustering on the dataset. In this assignment we will be using the python library \n",
      "[scikit-learn](http://scikit-learn.org/stable/index.html) for our machine learning algorithms. scikit-learn provides an extensive\n",
      "library of machine learning algorithms that can be used for analysis. Here is a [nice flow chart](http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html) that shows various algorithms implemented\n",
      "and when to use any of them. In this part of the assignment we will look at K-Means clustering\n",
      "\n",
      "> **Note on terminology**: \"samples\" and \"features\" are two words you will come across frequently when you look at machine learning papers or documentation. \"samples\" refer to data points that are used as inputs to the machine learning algorithm. For example in our dataset each artist is a \"sample\". \"features\" refers to some representation we have for every sample. For example the list of 1s and 0s we have for each artist are \"features\". Similarly the bag-of-words approach from the previous homework produced \"features\" for each document.\n",
      "\n",
      "#### K-Means algorithm\n",
      "\n",
      "Clustering is the process of automatically grouping data points that are similar to each other. In the [K-Means algorithm](http://en.wikipedia.org/wiki/K-means_clustering) we start with `K` initially chosen cluster centers (or centroids). We then compute the distance of every point from the centroids and assign each point to the centroid. Next we update the centroids by averaging all the points in the cluster. Finally, we repeat the algorithm until the cluster centers are stable.\n",
      "\n",
      "### Running K-Means\n",
      "\n",
      "#### K-Means interface\n",
      "Take a minute to look at the scikit-learn interface for calling [KMeans](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html). The constructor of the KMeans class returns a `estimator` on which you can call [fit](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans.fit) to perform clustering.\n",
      "\n",
      "#### K-Means parameters\n",
      "From the above description we can see that there are a few parameters which control the K-Means algorithm. We will look at one parameter specifically, the number of clusters used in the algorithm. The number of clusters needs to be chosen based on domain knowledge of the data. As we do not know how many genres exist we will try different values and compare the results.\n",
      "\n",
      "#### Timing your code\n",
      "We will also measure the performance of clustering algorithms in this section. You can time the code in a cell using the **%%time** [IPython magic](http://nbviewer.ipython.org/github/ipython/ipython/blob/1.x/examples/notebooks/Cell%20Magics.ipynb) as the first line in the cell. \n",
      "\n",
      ">**Note**: By default, the scikit-learn KMeans implementation runs the algorithm 10 times with different center initializations. For this assignment you can run it just once by passing the `n_init` argument as 1."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Exercise 2\n",
      "\n",
      "**a**. Run K-means using *5* cluster centers on the `user_np_matrix`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "# Run K-means using 5 cluster centers on user_np_matrix\n",
      "kmeans_5 = KMeans(n_clusters = 5, n_init=1)\n",
      "kmeans_5.fit(user_np_matrix)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CPU times: user 6.73 s, sys: 896 ms, total: 7.63 s\n",
        "Wall time: 5.75 s\n"
       ]
      }
     ],
     "prompt_number": 145
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**b**. Run K-means using *25* and *50* cluster centers on the `user_np_matrix`. Also measure the time taken for both cases."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "kmeans_25 = KMeans(n_clusters = 25, n_init=1)\n",
      "kmeans_25.fit(user_np_matrix)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CPU times: user 10.4 s, sys: 1.28 s, total: 11.7 s\n",
        "Wall time: 8.07 s\n"
       ]
      }
     ],
     "prompt_number": 146
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "kmeans_50 = KMeans(n_clusters = 50, n_init=1)\n",
      "kmeans_50.fit(user_np_matrix)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CPU times: user 13.1 s, sys: 1.55 s, total: 14.6 s\n",
        "Wall time: 9.28 s\n"
       ]
      }
     ],
     "prompt_number": 147
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**d**. Of the three algorithms, which setting took the longest to run ? Why do you think this is the case ?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> The 50 means took the longest. Looking at how the algorithm terminates, we learn that it terminates when each data points converge to it's final centroid. Given that 5 and 25 terminated faster than 50, I'd make the conjectuer that the 50 centroids took a longer time to converge; this is probaly caused by the fact that more centroid means more choices for each data points to shift towards. This resulted in a longer convergence time."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 1.3 Evaluating K-Means\n",
      "\n",
      "In addition to the performance comparisons we also wish to compare how good our clusters are. To do this we are first going to look at internal evaluation metrics. For internal evaluation we only use the input data and the clusters created and try to measure the quality of clusters created. We are going to use two metrics for this:\n",
      "\n",
      "#### Inertia\n",
      "Inertia is a metric that is used to estimate how close the data points in a cluster are. This is calculated as the sum of squared distance for each point to it's closest centroid, i.e., its assigned cluster center. The intution behind inertia is that clusters with lower inertia are better as it means closely related points form a cluster.Inertia is calculated by scikit-learn by default.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Exercise 3**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**a**. Print inertia for all the kmeans model computed above."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Inertia for KMeans with 5 clusters = %lf \" % kmeans_5.inertia_\n",
      "print \"Inertia for KMeans with 25 clusters = %lf \" % kmeans_25.inertia_\n",
      "print \"Inertia for KMeans with 50 clusters = %lf \" % kmeans_50.inertia_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Inertia for KMeans with 5 clusters = 349814.960539 \n",
        "Inertia for KMeans with 25 clusters = 321495.797075 \n",
        "Inertia for KMeans with 50 clusters = 309379.187033 \n"
       ]
      }
     ],
     "prompt_number": 158
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**b**. Does KMeans run with 25 clusters have lower or greater inertia than the ones with 5 clusters ? Which algorithm is better and why ?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      ">The 25 cluster had lower inertia. This tells us that the 25 clusters had - on average - more points that were closer to its respectie centrion. Technically, this tells us that the 25 cluster fit the data better. However, I am reluctant to conclude in such way, because we know that inertia is bound to drop as we increase cluster count. Ultimately, if we have as many cluter as data points, we would have an inertia of 0. Though the 25 cluster seems to fit better, it is hard to conclude that it is \"better\"."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Silhouette Score: \n",
      "The silhouette score measures how close various clusters created are. A higher silhouette score is better as it means that we dont have too many overlapping clusters. The silhouette score can be computed using [sklearn.metrics.silhouette_score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html#sklearn.metrics.silhouette_score) from scikit learn."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**c.** Calculate the Silhouette Score using 500 sample points for all the kmeans models."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import silhouette_score\n",
      "\n",
      "# NOTE: Use 500 sample points to calculate the silhouette score\n",
      "def get_silhouette_score(data, model):\n",
      "    return silhouette_score(data, model.labels_,sample_size=500)\n",
      "\n",
      "\n",
      "print \"Silhouette Score for KMeans with 5 clusters = %lf\" % get_silhouette_score(user_np_matrix, kmeans_5)\n",
      "print \"Silhouette Score for KMeans with 25 clusters = %lf \" % get_silhouette_score(user_np_matrix, kmeans_25)\n",
      "print \"Silhouette Score for KMeans with 50 clusters = %lf \" % get_silhouette_score(user_np_matrix, kmeans_50)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Silhouette Score for KMeans with 5 clusters = 0.266919\n",
        "Silhouette Score for KMeans with 25 clusters = 0.082100 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Silhouette Score for KMeans with 50 clusters = 0.056025 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 156
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**d**. How does increasing the number of clusters affect the silhouette score ?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> More cluster seems to lead to higher silhouette score."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 1.4 External Evaluation\n",
      "While internal evaluation is useful, a better method for measuring clustering quality is to do external evaluation. This might not be possible always as we may not have ground truth data available. In our application we will use `top_tags` from before as our ground truth data for external evaluation. We will first compute purity and accuracy and finally we will predict tags for our **test** dataset.\n",
      "\n",
      "#### Exercise 4\n",
      "\n",
      "**a**. As a first step we will need to **join** the `artist_tags` data with the set of labels generated by K-Means model. That is, for every artist we will now have the top tag, cluster id and artist name in a data structure."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "\n",
      "# Return a data structure that contains artist_id, artist_name, top tag, cluster_label for every artist\n",
      "def join_tags_labels(artists_data, user_data, kmeans_model):\n",
      "    res = pd.DataFrame(columns=('ArtistID', 'ArtistName', 'TopTag', 'ClusterLabel'))\n",
      "    for artist_tup, top_tag in artists_data.items():\n",
      "        artist_id = artist_tup[0]\n",
      "        artist_name = artist_tup[1]\n",
      "        prediction_data = user_data[user_data[\"ArtistID\"] == artist_id]\n",
      "        pred_matrix = prediction_data.as_matrix()\n",
      "        pred_matrix = np.delete(pred_matrix,0,1) #remove first col\n",
      "        row = pd.DataFrame([{\"ArtistID\":artist_id, \"ArtistName\":artist_name, \"TopTag\":top_tag, \"ClusterLabel\":kmeans_model.predict(pred_matrix)}])\n",
      "        res = pd.concat([res, row])\n",
      "        #res = res.append(row, ignore_index=True)\n",
      "    return res\n",
      "\n",
      "# Run the function for all the models\n",
      "kmeans_5_joined = join_tags_labels(top_tags, user_art_mat ,kmeans_5)\n",
      "kmeans_25_joined = join_tags_labels(top_tags, user_art_mat ,kmeans_25)\n",
      "kmeans_50_joined = join_tags_labels(top_tags, user_art_mat ,kmeans_50)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CPU times: user 4min 9s, sys: 900 ms, total: 4min 9s\n",
        "Wall time: 4min 9s\n"
       ]
      }
     ],
     "prompt_number": 225
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print kmeans_5_joined.head(100)\n",
      "#TODO REMOVE"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "                                ArtistID                     ArtistName  \\\n",
        "0   dd14a6a3-7055-410b-bb2b-8ec2d8ff5c42                    Jimmy Bosch   \n",
        "1   369cbaed-5209-4990-83c9-f8233d6c248b                   Enuff Z'Nuff   \n",
        "2   9c17713b-14f7-47ee-b0ba-87802d186aab                    Brant Bjork   \n",
        "3   e3a3c55a-b95e-47e9-b4cb-7bfd63c51bfa         Mouth of the Architect   \n",
        "4   88b846e0-faf3-40f9-a8eb-39d438ea5a3b         My Awesome Compilation   \n",
        "5   ca59a38e-cf39-4d95-ac03-b6ca4f43fd8f                  Edyta G\u00f3rniak   \n",
        "6   1d3da898-4fb4-4445-b704-cd7315e01789                      Sunn O)))   \n",
        "7   475b7312-4fcb-49a4-8bcc-cf3a679b255a                     Joe Dassin   \n",
        "8   e8c70743-2890-402b-bc49-539b609a33bb                         Boikot   \n",
        "9   5a63c831-5bdb-444d-a9db-974501e3317d                          Sloan   \n",
        "10  d381d6bd-e46f-466a-af5f-8faaa6b76b3a                     No Respect   \n",
        "11  ea075268-e5ea-40f7-b9c3-68b039218a88           The Apples in Stereo   \n",
        "12  b8efe8ac-45db-4a71-bb51-c1503986172f                   Arlo Guthrie   \n",
        "13  a96ac800-bfcb-412a-8a63-0a98df600700                   Modest Mouse   \n",
        "14  f7f0a03c-edd4-4791-8d81-84fa01afda22  Three Dead Trolls in a Baggie   \n",
        "15  333c8de5-d751-436a-a803-a1267f82f1fe                    Micah Green   \n",
        "16  d671b443-79d5-4995-885d-e7d7410d2d62                           ZONE   \n",
        "17  165a9e64-b146-43fd-aa81-98734a38bdee                Matty Pop Chart   \n",
        "18  fca323ff-d937-489c-957b-d3339ae7ee12                       Agonizer   \n",
        "19  a87bdb2c-025f-478b-b337-ffe763f23e42                          Twine   \n",
        "20  12a8ea6c-8eea-4057-ab78-2a35fa2ac3bc                    Green Court   \n",
        "21  a5a38385-5120-4ff8-a2fd-16639da732f6                    Null Device   \n",
        "22  b73ad3ac-be50-4b3c-b5dd-7412d557602c                     Dan Reeder   \n",
        "23  a4d62ced-2f87-488a-9c76-ab88ea7a2cbf                 Pharoahe Monch   \n",
        "24  dcd95aa4-f3ff-492d-a621-8518ada0f58b                          Falco   \n",
        "25  190a2c7a-db32-4564-9b32-41951091ccb3                          DJ GT   \n",
        "26  2119beb8-6ac5-4f21-82a4-b831c90c0024                    Damien Rice   \n",
        "27  bbd05acc-33f0-4db6-bc01-41c8b4bfe2df                No te va Gustar   \n",
        "28  0336e885-1b2b-4f3c-9886-e4921a34b615                    Enrico Rava   \n",
        "29  2f4cc8b7-bf39-402e-b9c9-1c7ca1de2efd                 John Barrowman   \n",
        "30  28e56e59-1cc7-4607-84d4-a8dacbfab5e3         Blizzard Entertainment   \n",
        "31  7aa52f4c-0d13-4d45-94f1-9e84200b7496                           \u5965\u4e95\u96c5\u7f8e   \n",
        "32  c3bf5aee-646e-4030-9ab4-d1fbd4947a97                      The Veils   \n",
        "33  103b58fb-26ef-4c66-9b48-6eda02763fc8                     Vandenberg   \n",
        "34  4043e97e-8a69-424f-bf59-16bc8aac71e6                     Remy Shand   \n",
        "35  eeaaa16c-3d72-4031-9b9f-456eb4e70ed7                          Brace   \n",
        "36  547887e2-d2e9-4131-950a-fdfa0a155651                  Ashbury Faith   \n",
        "37  886557b3-2fa6-494e-993d-b955e789dc57                   Lori McKenna   \n",
        "38  196eb856-9b51-44c4-a188-e6ac244e77fe                    DJ Sharpnel   \n",
        "39  a12303fc-680f-4d8f-845b-740fd7fdfe45                  Rat Cat Hogan   \n",
        "40  490af4aa-21c2-4d47-97df-ecac964a0577                         Q-Unit   \n",
        "41  6547d3e4-8f1c-4453-bcc3-ad71a31d1259                            \u68ee\u4fca\u4e4b   \n",
        "42  ff294730-0315-440d-a543-54005779c15b          The Mamas & the Papas   \n",
        "43  505b403a-ebb8-4333-bed6-76ae3f413e17                         Nu NRG   \n",
        "44  0f80ab7f-6ba8-4a31-91c5-3a599e49bd1b                      The Doers   \n",
        "45  dca3c0f1-daa7-4a3d-9c1d-1daa4bf1fac5               Paula Koivuniemi   \n",
        "46  c66b8b18-5ce8-4f60-ae56-3aa810761cc0     ROUND TABLE featuring Nino   \n",
        "47  e6625306-6e56-4851-bbb7-3259db088c71                         Mudmen   \n",
        "48  ff930e2c-6a6f-40a8-9bde-50627e448773                      Buffseeds   \n",
        "49  de1bbff3-602a-4c55-a232-117f088889df                          Lumsk   \n",
        "50  c139d092-9457-40ec-8130-e11d4cbcfb56                       Bill Fay   \n",
        "51  64b664c3-60a9-4880-8908-bfd3308c1c85                     With Honor   \n",
        "52  1b936c03-2871-4d19-8bad-b4613eafd15a                       Joniveli   \n",
        "53  9904bde1-04a7-41cf-a2a3-d09a42445145                   Scatman John   \n",
        "54  47f67b22-affe-4fe1-9d25-853d69bc0ee3                    Dave Dobbyn   \n",
        "55  8bc8de2d-ee8a-4cb4-85c6-fac1c9f007a5       Triosk meets Jan Jelinek   \n",
        "56  3717ecb5-519f-4c6a-92c0-1a393a99fed3                      Reflexion   \n",
        "57  2b81fe9b-056d-43b2-b633-356295230671                    Whyte Seeds   \n",
        "58  b45335d1-5219-4262-a44d-936aa36eeaed                       Ladytron   \n",
        "59  de2f6b7a-9d08-4a16-88c1-68c141e0b0c6               \u0412\u044f\u0447\u0435\u0441\u043b\u0430\u0432 \u0411\u0443\u0442\u0443\u0441\u043e\u0432   \n",
        "                                     ...                            ...   \n",
        "\n",
        "   ClusterLabel             TopTag  \n",
        "0            []              salsa  \n",
        "1            []               rock  \n",
        "2           [3]        Stoner Rock  \n",
        "3            []             Sludge  \n",
        "4            []          seen live  \n",
        "5           [1]             polish  \n",
        "6           [3]              drone  \n",
        "7           [3]             french  \n",
        "8            []               punk  \n",
        "9           [3]           Canadian  \n",
        "10           []                ska  \n",
        "11          [4]              indie  \n",
        "12          [1]               folk  \n",
        "13           []              indie  \n",
        "14           []             comedy  \n",
        "15           []        alternative  \n",
        "16           []           japanese  \n",
        "17           []          seen live  \n",
        "18          [1]          seen live  \n",
        "19           []                idm  \n",
        "20           []             trance  \n",
        "21           []         electronic  \n",
        "22          [1]               folk  \n",
        "23          [3]            Hip-Hop  \n",
        "24           []                80s  \n",
        "25           []             trance  \n",
        "26          [2]  singer-songwriter  \n",
        "27           []                ska  \n",
        "28           []               jazz  \n",
        "29          [1]           Broadway  \n",
        "30           []   game soundtracks  \n",
        "31          [1]           japanese  \n",
        "32           []          seen live  \n",
        "33           []          hard rock  \n",
        "34          [1]               soul  \n",
        "35           []          seen live  \n",
        "36           []            belgian  \n",
        "37          [1]  singer-songwriter  \n",
        "38           []     happy hardcore  \n",
        "39           []              indie  \n",
        "40           []             mashup  \n",
        "41           []               game  \n",
        "42          [4]       classic rock  \n",
        "43          [1]             trance  \n",
        "44           []           Canadian  \n",
        "45           []            finnish  \n",
        "46           []           japanese  \n",
        "47           []               rock  \n",
        "48          [1]              indie  \n",
        "49          [1]         folk metal  \n",
        "50           []               folk  \n",
        "51          [1]           hardcore  \n",
        "52           []                rap  \n",
        "53          [1]              dance  \n",
        "54           []               Kiwi  \n",
        "55          [3]               jazz  \n",
        "56          [1]            finnish  \n",
        "57          [1]          seen live  \n",
        "58          [2]         electronic  \n",
        "59           []       russian rock  \n",
        "            ...                ...  \n",
        "\n",
        "[100 rows x 4 columns]\n"
       ]
      }
     ],
     "prompt_number": 222
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Return a data structure that contains artist_id, artist_name, top tag, cluster_label for every artist\n",
      "'''\n",
      "def join_tags_labels(artists_data, user_data, kmeans_model):\n",
      "    result = {} # {tup(artist_id, artist_name), (top_tag, cluter_label)}\n",
      "    for artist_tup, top_tag in artists_data.items():\n",
      "        artist_id = artist_tup[0]\n",
      "        artist_name = artist_tup[1]\n",
      "        prediction_data = user_data[user_data[\"ArtistID\"] == artist_id]\n",
      "        pred_matrix = prediction_data.as_matrix()\n",
      "        pred_matrix = np.delete(pred_matrix,0,1) #remove first col\n",
      "        result[artist_tup] = (top_tag, kmeans_model.predict(pred_matrix))\n",
      "    return result\n",
      "'''\n",
      "a = kmeans_25_joined[kmeans_25_joined.keys()[2]]\n",
      "#print a\n",
      "#@TODO REMOVE"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 227
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**b**. Next we need to generate a genre for every cluster id we have (the cluster ids are from 0 to N-1). You can do this by **grouping** the data from the previous exercise on cluster id. \n",
      "\n",
      "One thing you might notice is that we typically get a bunch of different tags associated with every cluster. How do we pick one genre or tag from this ? To cover various tags that are part of the cluster, we will pick the **top 5** tags in each cluster and save the list of top-5 tags as the genre for the cluster.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Return a data structure that contains cluster_id, list of top 5 tags for every cluster\n",
      "def assign_cluster_tags(joined_data):\n",
      "    cluster_groups = {} #{cluter_id : {tag:count}}\n",
      "    for index, data in joined_data.iterrows():\n",
      "        top_tag = data[\"TopTag\"]\n",
      "        cluster_id = data[\"ClusterLabel\"]\n",
      "        try:\n",
      "            cluter_id = cluster_id[0]\n",
      "        except IndexError: #weird issue where we don't have lable, because of way back; we couldn't find prediction data.\n",
      "            cluster_id = -1\n",
      "            #continue #maybe ignore these?\n",
      "        cluster_tag_count = cluster_groups.get(cluster_id, {})\n",
      "        cluster_tag_count[top_tag] = cluster_tag_count.get(top_tag, 0) + 1\n",
      "        cluster_groups[cluster_id] = cluster_tag_count\n",
      "        break #@TODO REMOVE\n",
      "    for cluster, tag_list in cluster_groups.items():\n",
      "        sorted_tag_counts = sorted(tag_list.iteritems(), key=lambda (k, v): v, reverse=True)\n",
      "        largest_five = sorted_tag_counts #TODO FIX HERE to get top 5.\n",
      "        print largest_five\n",
      "        break #@TODO REMOVE\n",
      "    return cluster_groups\n",
      "\n",
      "kmeans_5_genres = assign_cluster_tags(kmeans_5_joined)\n",
      "kmeans_25_genres = None\n",
      "kmeans_50_genres = None"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('salsa', 1)]\n"
       ]
      }
     ],
     "prompt_number": 258
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Purity and Accuracy\n",
      "Two commonly used metrics used for evaluating clustering using external labels are purity and accuracy. **Purity** measures the frequency of data belonging to the same cluster sharing the same class label i.e. if we have a number of items in a cluster how many of those items have the same label ? Meanwhile, **accuracy** measures the frequency of data from the same class appearing in a single cluster i.e. of all the items which have a particular label what fraction appear in the same cluster ?\n",
      "\n",
      ">NOTE: This is similar to precision and recall that we looked at in the previous assignment. Purity here makes sure that our clusters have mostly homogeneous labels while accuracy make sure that our labels are not spread out over too many clusters."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**d**. Compute the purity for each of our K-Means models. To do this find the top tags of all artists that belong to a cluster. Check what fraction of these tags are covered by the top 5 tags of the cluster. Average this value across all clusters. **HINT**: We used similar ideas to get the top 5 tags in a cluster. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_cluster_purity(joined_data):\n",
      "    pass\n",
      "    \n",
      "print \"Purity for KMeans with 5 centers %lf \" % 0.0\n",
      "print \"Purity for KMeans with 25 centers %lf \" % 0.0\n",
      "print \"Purity for KMeans with 50 centers %lf \" % 0.0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**e**. To compute the accuracy first get all the unique tags from *top_tags*. Then for each tag, compute how many artists are found in the largest cluster. We denote these as correct cluster assignments. For example, lets take a tag 'rock'. If there are 100 artists with tag 'rock' and say 90 of them are in one cluster while 10 of them are in another. Then we have 90 correct cluster assignments\n",
      "\n",
      "Add the number of correct cluster assignments for all tags and divide this by the total size of the training data to get the accuracy for a model."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_accuracy(joined_data):\n",
      "    pass\n",
      "    \n",
      "print \"Accuracy of KMeans with 5 centers %lf \" % 0.0\n",
      "print \"Accuracy of KMeans with 25 centers %lf \" % 0.0\n",
      "print \"Accuracy of KMeans with 50 centers %lf \" % 0.0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**f.** What do the numbers tell you about the models? Do you have a favorite?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      ">TODO: Your answer here."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 1.5 Evaluating Test Data\n",
      "Finally we can treat the clustering model as a multi-class classifier and make predictions on external test data. To do this we load the test data file **userart-mat-test.csv** and for every artist in the file we use the K-Means model to predict a cluster. We mark our prediction as successful if the artist's top tag belongs to one of the five tags for the cluster. \n",
      "\n",
      "#### Exercise 5"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**a** Load the testdata file and create a NumPy matrix named user_np_matrix_test."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "user_art_mat_test = parse_user_artists_matrix(DATA_PATH + \"/userart-mat-test.csv\")\n",
      "# NOTE: the astype(float) converts integer to floats here\n",
      "user_np_matrix_test = create_user_matrix(user_art_mat_test).astype(float)\n",
      "\n",
      "user_np_matrix_test.shape # Should be (1902, 846)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**b.** For each artist in the test set, call **[predict](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans.predict)** to get the predicted cluster. Join the predicted labels with test artist ids. Return 'artist_id', 'predicted_label' for every artist in the test dataset."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# For every artist return a list of labels\n",
      "def predict_cluster(test_data, test_np_matrix, kmeans_model):\n",
      "    pass\n",
      "\n",
      "# Call the function for every model from before\n",
      "kmeans_5_predicted = None\n",
      "kmeans_25_predicted = None\n",
      "kmeans_50_predicted = None"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**c**. Get the tags for the predicted genre and the tag for the artist from `top_tags`. Output the percentage of artists for whom the top tag is one of the five that describe its cluster. This is the *recall* of our model.\n",
      ">NOTE: Since the tag data is not from the same source as user plays, there are artists in the test set for whom we do not have top tags. You should exclude these artists while making predictions and while computing the recall."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Calculate recall for our predictions\n",
      "def verify_predictions(predicted_artist_labels, cluster_genres, top_tag_data):\n",
      "    pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**d**. Print the recall for each KMeans model. We define recall as num_correct_predictions / num_artists_in_test_data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Use verify_predictions for every model\n",
      "print \"Recall of KMeans with 5 centers %lf \" % 0.0\n",
      "print \"Recall of KMeans with 25 centers %lf \" % 0.0\n",
      "print \"Recall of KMeans with 50 centers %lf \" % 0.0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 1.5 Visualizing Clusters using PCA\n",
      "\n",
      "Another way to evaluate clustering is to visualize the output of clustering. However the data we are working with is in 846 dimensions !, so it is hard to visualize or plot this. Thus the first step for visualization is to reduce the dimensionality of the data. To do this we can use [Prinicipal Component Analysis (PCA)](http://en.wikipedia.org/wiki/Principal_component_analysis). PCA reduces the dimension of data and keeps only the most significant components of it. This is a commonly used technique to visualize data from high dimensional spaces.\n",
      "\n",
      ">**NOTE**: We use [RandomizedPCA](http://scikit-learn.org/stable/modules/decomposition.html#approximate-pca), an approximate version of the algorithm as this has lower memory requirements. The approximate version is good enough when we are reducing to a few dimensions (2 in this case). We also sample the input data before PCA to further reduce memory requirements.\n",
      "\n",
      "#### Exercise 6\n",
      "\n",
      "**a**. Calcluate the RandomizedPCA of the sampled training data set `sampled_data` and reduce it to 2 components. Use the [fit_transform](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.RandomizedPCA.html#sklearn.decomposition.RandomizedPCA.fit_transform) method to do this."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.decomposition import RandomizedPCA\n",
      "import numpy as np\n",
      "\n",
      "sample_percent = 0.20\n",
      "rows_to_sample = int(np.ceil(sample_percent * user_np_matrix.shape[0]))\n",
      "sampled_data = input_data[np.random.choice(input_data.shape[0], rows_to_sample, replace=False),:]\n",
      "\n",
      "# Return the data reduced to 2 principal components\n",
      "def get_reduced_data(input_data):\n",
      "    pass\n",
      "\n",
      "user_np_2d = get_reduced_data(sampled_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**b**. Fit the reduced data with the KMeans model with 5 cluster centers. Plot the cluster centers and all the points. Make sure to color points in every cluster differently to see a visual separation. You may find [`scatter`](http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.scatter) and [`plot`](http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.plot) functions from matplotlib to be useful.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO: Write code to fit and plot reduced_data.\n",
      "import pylab as pl\n",
      "%pylab inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Part 2 - Regression Models - Predicting Song Popularity\n",
      "\n",
      "In this section of the assignment you'll be building a model to predict the number of plays a song will get. Again, we're going to be using scikit-learn to train and evaluate regression models, and pandas to pre-process the data.\n",
      "\n",
      "In the process, you'll encounter some modeling challenges and we'll look at how to deal with them.\n",
      "\n",
      "We've started with the same data as above, but this time we've pre-computed a number of song statistics for you.\n",
      "\n",
      "These are:\n",
      "\n",
      "1. plays - the number of times a song has been played.\n",
      "1. pctmale - percentage of the plays that came from users who self-identified as \"male\".\n",
      "1. age - average age of the listener.\n",
      "1. country1 - the country of the users that listened to this song most.\n",
      "1. country2 - the country of the users that listened to this song second most.\n",
      "1. country3 - the country of the users that listened to this song third most.\n",
      "1. pctgt1 - Percentage of plays that come from a user who's played the song more than once.\n",
      "1. pctgt2 - Percentage of plays that come from a user who's played the song more than twice.\n",
      "1. pctgt5 - Percentage of plays that come from a user who's played the song more than five times.\n",
      "1. cluster - The \"cluster number\" of the artist associated with this song - similar to what you came up with above. We chose 25 clusters fairly arbitrarily.\n",
      "\n",
      "### 2.1 Data Exploration\n",
      "#### Exercise 7\n",
      "\n",
      "**a**. Let's start by loading up the data - we've provided a \"training set\" and a \"validation set\" for you to test your models on. The training set are the examples that we use to create our models, while the validation set is a dataset we \"hold out\" from the model fitting process, we use these examples to test whether our models accurately predict new data.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline\n",
      "import pandas as pd\n",
      "\n",
      "train = pd.read_csv(DATA_PATH + \"/train_model_data.csv\")\n",
      "validation = pd.read_csv(DATA_PATH + \"/validation_model_data.csv\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that you've got the data loaded, play around with it, generate some descriptive statistics, and get a feel for what's in the data set. For the categorical variables try pandas \".count_values()\" on them to get a sense of the most likely distributions (countries, etc.). \n",
      "\n",
      "**b**. In the next cell put some commands you ran to get a feel for the data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO: Your commands for data exploration here."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**c**. Next, create a pairwise scatter plot of the columns: plays, pctmale, age, pctgt1, pctgt2, pctgt5. (_Hint: we did this in lab!_)\n",
      "\n",
      "Do you notice anything about the data in this view? What about the relationship between plays and other columns?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO: Your commands to generate a scatter plot here."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      ">TODO: What do you notice about the data in this view? Write your answer here."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###2.2 Data Prep and Intro to Linear Regression\n",
      "\n",
      "*scikit-learn* does a number of things very well, but one of the things it doesn't handle easily is categorical or missing data. Categorical data is data that can take on a finite set of values, e.g. a categorical variable might be the color of a stop light (Red, Yellow, Green), this is in contrast with continuous variables like real numbers in the range -Infinity to +Infinity. There is another common type of data called \"ordinal\" that can be thought of as categorical data that has a natural ordering, like: Cold, Warm, Hot. We won't be dealing with this kind of data here, but having that kind of ranking opens up the use of certain other statistical methods.\n",
      "\n",
      "\n",
      "#### Exercise 8\n",
      "\n",
      "**a**. For the first part of the exercise, let's eliminate categorical variables, and *impute* missing values with pandas. Write a function to drop all categorical variables from the data set, and return two pandas data frames:\n",
      "\n",
      "1. A data frame with all categorical items and a user-specified response column removed.\n",
      "2. A data frame that contains only the response column."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def basic_prep(data, col):\n",
      "    #TODO - make a copy of the original dataset but with the categorical variables removed! *Cluster* should be thought of as a \n",
      "    #categorical variable and should be removed! Make use of pandas \".drop\" function.\n",
      "    \n",
      "    #TODO - impute missing values with the mean of those columns, use pandas \".fillna\" function to accomplish this.\n",
      "    \n",
      "    pass\n",
      "\n",
      "#This will create two new data frames, one that contains training data - in this case all the numeric columns,\n",
      "#and one that contains response data - in this case, the \"plays\" column.\n",
      "train_basic_features, train_basic_response = basic_prep(train, 'plays')\n",
      "validation_basic_features, validation_basic_response = basic_prep(validation, 'plays')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, we're going to train a linear regression model. This is likely the most widely used model for fitting data out there today - you've probably seen it before, maybe even used it in Excel. The goal of linear modeling, is to fit a **linear equation** that maps a set of **input features** to a numerical **response**. This equation is called a **model**, and can be used to make predictions about the response of similar input features. For example, imagine we have a dataset of electricity prices ($p$) and outdoor temperature ($t$), and we want to predict, given temperature, what electricity price will be. A simple way to model this is with an equation that looks something like $p = basePrice + factor*t$. When we **fit** a model, we are estimating the parameters ($basePrice$ and $factor$) that best fit our data. This is a very simple linear model, but you can easily imagine extending this to situations where you need to estimate several parameters.\n",
      "\n",
      ">**Note**: It is possible to fill a semester with linear models (and classes in other departments do!), and there are innumerable issues to be aware of when you fit linear models, so this is just the tip of the iceberg - don't dismiss linear models outright based on your experiences here!\n",
      "\n",
      "A linear model models the data as a **linear combination** of the model and its weights. Typically, the model is written with something like the following form: $y = X\\theta + \\epsilon$, and when we fit the model, we are trying to find the value of $\\theta$ that minimizes the **loss** of the model. In the case of regression models, the loss is often represented as $\\sum (y - X\\theta)^2$ - or the squared distance between the prediction and the actual value.\n",
      "\n",
      "In the code below, `X` refers to the the training features, `y` refers to the training response, `Xv` refers to the validation features and yv refers to the validation response. Note that `X` is a matrix (or a `DataFrame`) with the shape $n \\times d$ where $n$ is the number of examples and $d$ is the number of features in each example, while `y` is a vector of length $n$ (one response per example).\n",
      "\n",
      "Our goal with this assignment is to accurately estimate the number of plays a song will get based on the features we know about it.\n",
      "\n",
      "The score we'll be judging the models on is called $R^2$, which is a measure of how well the model fits the data. It can be thought of roughly as the percentage of the variance that the model explains. \n",
      "\n",
      "#### Exercise 9\n",
      "\n",
      "**a.** Fit a `LinearRegression` model with scikit-learn and return the model score on both the training data and the validation data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import linear_model\n",
      "\n",
      "def fit_model(X, y):\n",
      "    #TODO - Write a function that fits a linear model to a dataset given a column of values to predict.\n",
      "    pass\n",
      "\n",
      "def score_model(model, X, y, Xv, yv):\n",
      "    #TODO - Write a function that returns scores of a model given its training \n",
      "    #features and response and validation features and response. \n",
      "    #The output should be a tuple of two model scores.\n",
      "    pass\n",
      "\n",
      "def fit_model_and_score(data, response, validation, val_response):\n",
      "    #TODO - Given a training dataset, a validation dataset, and the name of a column to predict, \n",
      "    #Using the model's \".score()\" method, return the model score on the training data *and* the validation data\n",
      "    #as a tuple of two doubles.\n",
      "    pass\n",
      "    #END TODO\n",
      "\n",
      "print fit_model_and_score(train_basic_features, train_basic_response, validation_basic_features, validation_basic_response)\n",
      "\n",
      "model = fit_model(train_basic_features, train_basic_response)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We realize that this may be your first experience with linear models - but that's a pretty low $R^2$ - we're looking for scores significantly higher than 0, and the maximum is a 1. \n",
      "\n",
      "So what happened? Well, we've modeled a **linear** response to our input features, but the variable we're modeling (plays) clearly has a non-linear relationship with respect to the input features. It roughly follows a **power-law** distribution, and so modeling it in linear space yields a model with estimates that are way off.\n",
      "\n",
      "We can verify this by looking at a plot of the model's residuals - that is, the difference between the training responses and the predictions. A good model would have residuals with two properties:\n",
      "\n",
      "1. Small in absolute value. \n",
      "1. Evenly distributed about the true values.\n",
      "\n",
      "**b.** Write a function to calculate the residuals of the model, and plot those with a histogram.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def residuals(features, y, model):\n",
      "    #TODO - Write a function that calculates model residuals given input features, ground truth, and the model.\n",
      "    pass\n",
      "\n",
      "#TODO - Plot the histogram of the residuals of your current model."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "See the structure in the plot? This means we've got more modeling to do before we can call it a day! It satisfies neither of our properties - we're often way wrong with our predictions, and seem to systematically **under** predict the number of plays a song will get.\n",
      "\n",
      "What happens if we try and predict the $log$ of number of plays? This controls the exponential behaviour of plays, and gives less weight to the case where our prediction was off by 100 when the true answer was 1000. \n",
      "\n",
      "#### Exercise 10\n",
      "**a.** Adapt your model fitting from above to fit the **log** of the nubmer of plays as your response variable. Print the scores."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import linear_model\n",
      "\n",
      "#TODO - Using what you built above, build a model using the log of the number of plays as the response variable.\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**b.** You should see a significantly better $R^2$ and validation $R^2$, though still pretty low. Take a look at the model residuals again, do they look any better?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#TODO Plot residuals of your log model. Note - we want to see these on a \"plays\" scale, not a \"log(plays)\" scale!"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There must be something we can do here to build a better model. Let's try incorporating country and cluster information.\n",
      "\n",
      "###2.3 Linear Modeling with Categorical Variables: One-Hot Encoding\n",
      "\n",
      "Linear models expect **numbers** for input features. But we have some features that we think could be useful that are **discrete** or **categorical**. How do we represent these as numbers?\n",
      "\n",
      "One solution is something called one-hot encoding. Basically, we map a discrete space to a vector of binary indicators, then use these indicators as numbers.\n",
      "\n",
      "For example, if I had an input column that could take on the values {$RED$, $GREEN$, $BLUE$}, and I wanted to model this with one-hot-encoding, I could use a map:\n",
      "\n",
      "* $RED = 001$\n",
      "* $GREEN = 010$\n",
      "* $BLUE = 100$\n",
      "\n",
      "We use this representation instead of traditional binary numbers to keep these features independent of one another.\n",
      "\n",
      "Once we've established this representation, we replace the columns in our dataset with their one-hot-encoded values. Then, we can fit a linear model on the data once it's encoded this way!\n",
      "\n",
      "Statisticians and econometricians call these types of binary variables *dummy variables*, but we're going to call it one-hot encoding, because that sounds cooler.\n",
      "\n",
      "Scikit-learn has functionality to transform values to this encoding built in, so we'll leverage that. The functionality is called `DictVectorizer` in scikit-learn. The idea is that you feed a `DictVectorizer` a bunch of examples of your data (that's the `vec.fit` line), and it builds a map from a categorical variable to a one-hot encoded vector like we have in the color example above. Then, you can use this object to translate from categorical values to sequences of numeric ones as we do with `vec.transform`. In the example below, we fit a vectorizer on the training data and use the same vectorizer on the validation data so that the mapping is consistent and we don't run into issues if the categories don't match perfectly between the two data sets.\n",
      "\n",
      "####Exercise 11\n",
      "**a.** Use the code below to generate new training and validation datasets for the datasets *with* the categorical features in them."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import feature_extraction\n",
      "\n",
      "def one_hot_dataframe(data, cols, vec=None):\n",
      "    \"\"\" Takes a dataframe and a list of columns that need to be encoded.\n",
      "        Returns a tuple comprising the data, and the fitted vectorizor.\n",
      "        \n",
      "        Based on https://gist.github.com/kljensen/5452382\n",
      "    \"\"\"\n",
      "    if vec is None:\n",
      "        vec = feature_extraction.DictVectorizer()\n",
      "        vec.fit(data[cols].to_dict(outtype='records'))\n",
      "    \n",
      "    vecData = pd.DataFrame(vec.transform(data[cols].to_dict(outtype='records')).toarray())\n",
      "    vecData.columns = vec.get_feature_names()\n",
      "    vecData.index = data.index\n",
      "    \n",
      "    data = data.drop(cols, axis=1)\n",
      "    data = data.join(vecData)\n",
      "    return (data, vec)\n",
      "\n",
      "def prep_dset(data, col, vec=None):\n",
      "    #Convert the clusters to strings.\n",
      "    new_data = data\n",
      "    new_data['cluster'] = new_data['cluster'].apply(str)\n",
      "    \n",
      "    #Encode the data with OneHot Encoding.\n",
      "    new_data, vec = one_hot_dataframe(new_data, ['country1','cluster'], vec)\n",
      "\n",
      "    #Eliminate features we don't want to use in the model.\n",
      "    badcols = ['country2','country3','artid','key','age']\n",
      "    new_data = new_data.drop(badcols, axis=1)\n",
      "    \n",
      "    new_data = new_data.fillna(new_data.mean())\n",
      "    \n",
      "    return (new_data.drop([col], axis=1), pd.DataFrame(new_data[col]), vec)\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_cats_features, train_cats_response, vec = prep_dset(train, 'plays')\n",
      "validation_cats_features, validation_cats_response, _ = prep_dset(validation, 'plays', vec)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**b.** Now that you've added the categorical data, let's see how it works with a linear model!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print fit_model_and_score(train_cats_features, train_cats_response, validation_cats_features, validation_cats_response)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You should see a much better $R^2$ for the training data, but a much *worse* one for the validation data. What happened?\n",
      "\n",
      "This is a phenomenon called **overfitting** - our model has too many degrees of freedom (one parameter for each of the 100+ features of this dataset. This means that while our model fits the training data reasonably well, but at the expense of being too specific to that data.\n",
      "\n",
      "John Von Neumann famously said [\"With four parameters I can fit an elephant, and with five I can make him wiggle his trunk!\"](http://www.johndcook.com/blog/2011/06/21/how-to-fit-an-elephant/).\n",
      "\n",
      "### 2.4 Non-linear modeling and Regression Trees\n",
      "So, we're at an impasse. We didn't have enough features and our model performed poorly, we added too many features and our model looked good on training data, but not so good on test data.\n",
      "\n",
      "What's a modeler to do? \n",
      "\n",
      "There are a couple of ways of dealing with this situation - one of them is called **regularization**, which you might try on your own (see `RidgeRegression` or `LassoRegression` in scikit-learn), another is to use a model which captures **non-linear** relationships between the features and the response variable.\n",
      "\n",
      "One such type of model was pioneered here at Berkeley, by the late, great Leo Breiman. These models are called **regression trees**.\n",
      "\n",
      "The basic idea behind regression treees is to recursively partition the dataset into subsets that are *similar with respect to the response variable*. \n",
      "\n",
      "If we take our temperature example, we might observe a non-linear relationship - electricity gets expensive when it's cold outside because we use the heater, but it also gets expensive when it's too hot outside because we run the air conditioning. \n",
      "\n",
      "A decision tree model might dynamically elect to split the data on the temperature feature, and estimate high prices both for hot and cold, with lower prices for more Berkeley-like temperatures. Go read the [scikit-learn decision trees documentation](http://scikit-learn.org/stable/modules/tree.html) for more background.\n",
      "\n",
      "####Exercise 12\n",
      "**a.** Using the scikit learn `DecsionTreeRegressor` API, write a function that fits trees with the parameter 'max_depth' exposed to the user, and set to 10 by default."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import tree\n",
      "\n",
      "def fit_tree(X, y, depth=10):\n",
      "    ##TODO: Using the DecisionTreeRegressor, train a model to depth 10.\n",
      "    pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**b.** You should be able to use your same scoring function as above to compute your model scores. Write a function that fits a tree model to your training set and returns the model's score for both the training set and the validation set."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def fit_model_and_score_tree(train_features, train_response, val_features, val_response):\n",
      "    ##TODO: Fit a tree model and report the score on both the training set and test set.\n",
      "    pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**c.** Report the scores on the training and test data for both the basic features and the categorical features."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print fit_model_and_score_tree(train_basic_features, train_basic_response, validation_basic_features, validation_basic_response)\n",
      "print fit_model_and_score_tree(train_cats_features, train_cats_response, validation_cats_features, validation_cats_response)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Hooray - we've got a model that performs well on the training data set *and* the validation dataset. Which one is better? Why do you think that is. Try varying the depth of the decision tree (from, say, 2 to 20) and see how either data set does with respect to training and validation error. \n",
      "\n",
      "**d.** Now, let's build a tree to depth 3 and take a look at it."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import StringIO \n",
      "import pydot \n",
      "from IPython.display import Image\n",
      "\n",
      "tmodel = fit_tree(train_basic_features, train_basic_response, 3)\n",
      "\n",
      "def display_tree(tmodel):\n",
      "    dot_data = StringIO.StringIO() \n",
      "    tree.export_graphviz(tmodel, out_file=dot_data) \n",
      "    graph = pydot.graph_from_dot_data(dot_data.getvalue())\n",
      "    return Image(graph.create_png())\n",
      "    \n",
      "display_tree(tmodel)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**e.** What is the tree doing? It looks like it's making a decision on variables X[4] and X[2] - can you briefly describe, in words, what the tree is doing?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      ">TODO: Your answer goes here."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**f.** Finally, let's take a look at variable importance for a tree trained to 10 levels - this is a more formal way of deciding which features are important to the tree. The metric that scikit-learn calculates for feature importance is called GINI importance, and measures how much total 'impurity' is removed by splits from a given node. Variables that are highly discriminitive (e.g. ones that occur frequently throughout the tree) have higher GINI scores. You can read more about these scores [here](http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#giniimp)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tmodel = fit_tree(train_basic_features, train_basic_response, 10)\n",
      "\n",
      "pd.DataFrame(tmodel.feature_importances_, train_basic_features.columns)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**g.** What do you notice? Is the output interpretable? How would you explain this to someone?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      ">TODO: Your answer goes here."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}